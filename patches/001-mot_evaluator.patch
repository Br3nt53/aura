--- a/evaluation/mot_evaluator.py
+++ b/evaluation/mot_evaluator.py
@@ -1,26 +1,33 @@
 from __future__ import annotations
 from dataclasses import dataclass
 from typing import Dict, List, Tuple, Optional, Iterable
-import json, numpy as np
-from collections import defaultdict, Counter
+import json, numpy as np, os
+from collections import defaultdict, Counter
 
 try:
     from scipy.optimize import linear_sum_assignment
     _HAVE_SCIPY = True
 except Exception:
     _HAVE_SCIPY = False
 
 BBox = Tuple[float, float, float, float]
 Det  = Dict[str, object]
 
 @dataclass
 class EvalParams:
-    rf_weight: float = 0.0
-    wall_bonus: float = 0.0
-    track_decay_sec: float = 1.0
-    iou_threshold: float = 0.5   # when bboxes are available
+    rf_weight: float = 0.0
+    wall_bonus: float = 0.0
+    track_decay_sec: float = 1.0
+    iou_threshold: float = 0.5   # when bboxes are available
+    fps: float = 10.0            # frames/sec to convert seconds → frames
+    # Fragment boundary behavior:
+    #  - "strict":     fragment if gap > decay_frames
+    #  - "inclusive":  fragment if gap >= decay_frames
+    fragment_gap_mode: str = "strict"
@@
 def _hungarian(cost: np.ndarray):
     if cost.size == 0:
         return np.array([], dtype=int), np.array([], dtype=int)
@@
 class MOTEvaluator:
@@
-    def evaluate(self, pred_file: str, gt_file: str, *, scenario_path=None, workdir=None,
-                 rf_weight=None, wall_bonus=None, track_decay_sec=None) -> Dict[str, object]:
+    def evaluate(self, pred_file: str, gt_file: str, *, scenario_path=None, workdir=None,
+                 rf_weight=None, wall_bonus=None, track_decay_sec=None) -> Dict[str, object]:
         if rf_weight is not None:      self.params.rf_weight = float(rf_weight)
         if wall_bonus is not None:     self.params.wall_bonus = float(wall_bonus)
         if track_decay_sec is not None:self.params.track_decay_sec = float(track_decay_sec)
@@
-        # continuity state (decay)
-        fps = 1.0
-        decay_frames = max(1, int(round(self.params.track_decay_sec * fps)))
-        last_seen_frame = {}
-        last_pred_for_gt = {}
+        # continuity state (decay)
+        fps = float(getattr(self.params, "fps", 10.0))
+        decay_frames = max(1, int(round(self.params.track_decay_sec * fps)))
+        last_seen_frame = {}
+        last_pred_for_gt = {}
+        gt_fragments = Counter()      # temporal fragments counted by decay-gap
+        self._skipped_counts = {}     # file_basename -> skipped_line_count
@@
-            # IDSW + decay-based fragments
-            for gt_id, pred_id, _sim in matched:
-                # Fragment if resumed after decay gap
-                if gt_id in last_seen_frame and (t - last_seen_frame[gt_id]) > decay_frames:
-                    # we’ll count fragments from pred_to_gt_counts later; here continuity break matters for IDSW gating
-                    pass
-                # IDSW only within continuity window
-                prev_pid = last_pred_for_gt.get(gt_id)
-                if prev_pid is not None and prev_pid != pred_id and (t - last_seen_frame.get(gt_id, t)) <= decay_frames:
-                    sum_idsw += 1
-                last_seen_frame[gt_id] = t
-                last_pred_for_gt[gt_id] = pred_id
-
-                pred_to_gt_counts[pred_id][gt_id] += 1
-                gt_ever_tracked.add(gt_id)
+            # IDSW + decay-based fragments
+            for gt_id, pred_id, _sim in matched:
+                if gt_id in last_seen_frame:
+                    gap = t - last_seen_frame[gt_id]
+                    if self.params.fragment_gap_mode == "inclusive":
+                        is_fragment = (gap >= decay_frames)
+                    else:
+                        is_fragment = (gap > decay_frames)
+                    if is_fragment:
+                        gt_fragments[gt_id] += 1
+                # IDSW only within continuity window
+                prev_pid = last_pred_for_gt.get(gt_id)
+                if prev_pid is not None and prev_pid != pred_id and (t - last_seen_frame.get(gt_id, t)) <= decay_frames:
+                    sum_idsw += 1
+                last_seen_frame[gt_id] = t
+                last_pred_for_gt[gt_id] = pred_id
+                pred_to_gt_counts[pred_id][gt_id] += 1
+                gt_ever_tracked.add(gt_id)
@@
-        mota = 1.0 if total_gt == 0 else 1.0 - float(sum_fn + sum_fp + sum_idsw) / float(total_gt)
+        # Empty-data guard for MOTA
+        if total_gt == 0:
+            mota = 1.0 if (sum_fp + sum_fn + sum_idsw) == 0 else 0.0
+        else:
+            mota = 1.0 - float(sum_fn + sum_fp + sum_idsw) / float(total_gt)
         motp = float(np.mean(matched_ious)) if matched_ious else 0.0
@@
-        # track stats (no second matching pass)
+        # track stats (no second matching pass)
         purities = []
         for pid, ctr in pred_to_gt_counts.items():
             tot = sum(ctr.values())
             if tot > 0: purities.append(max(ctr.values())/tot)
         track_purity = float(np.mean(purities)) if purities else 0.0
         track_completeness = float(len(pred_to_gt_counts)) / float(len(all_pred_tracks)) if all_pred_tracks else 0.0
-
-        gt_to_pred_ids = defaultdict(set)
-        for pid, ctr in pred_to_gt_counts.items():
-            for gid in ctr.keys():
-                gt_to_pred_ids[gid].add(pid)
-        fragments = sum(max(0, len(pids)-1) for pids in gt_to_pred_ids.values())
-        id_fragmentation_rate = float(fragments) / float(max(1, len(gt_ever_tracked)))
+        # Temporal fragments via decay-gap
+        fragments = int(sum(gt_fragments.values()))
+        # Structural fragmentation via multiple pred IDs per GT
+        gt_to_pred_ids = defaultdict(set)
+        for pid, ctr in pred_to_gt_counts.items():
+            for gid in ctr.keys():
+                gt_to_pred_ids[gid].add(pid)
+        split_pairs = sum(max(0, len(pids) - 1) for pids in gt_to_pred_ids.values())
+        id_fragmentation_rate = float(split_pairs) / float(max(1, len(gt_ever_tracked)))
 
-        return {
+        meta = {"scenario": scenario_path, "workdir": workdir}
+        if getattr(self, "_skipped_counts", None):
+            meta["data_quality"] = {"skipped_lines": self._skipped_counts}
+
+        return {
             "mota": float(mota), "motp": float(motp), "auc": float(auc),
             "precision": float(precision), "recall": float(recall), "f1_score": float(f1),
             "id_switches": int(sum_idsw), "fragments": int(fragments),
             "track_purity": float(track_purity), "track_completeness": float(track_completeness),
             "id_fragmentation_rate": float(id_fragmentation_rate),
             "params": {
-                "rf_weight": self.params.rf_weight, "wall_bonus": self.params.wall_bonus,
-                "track_decay_sec": self.params.track_decay_sec, "iou_threshold": self.params.iou_threshold
+                "rf_weight": self.params.rf_weight, "wall_bonus": self.params.wall_bonus,
+                "track_decay_sec": self.params.track_decay_sec, "iou_threshold": self.params.iou_threshold,
+                "fps": self.params.fps, "fragment_gap_mode": self.params.fragment_gap_mode
             },
-            "meta": {"scenario": scenario_path, "workdir": workdir}
+            "meta": meta
         }
@@
-    def _read_jsonl(self, path: str):
-        with open(path, "r", encoding="utf-8") as f:
-            for line in f:
-                s = line.strip()
-                if not s: continue
-                yield json.loads(s)
+    def _read_jsonl(self, path: str):
+        skipped = 0
+        with open(path, "r", encoding="utf-8") as f:
+            for ln, line in enumerate(f, 1):
+                s = line.strip()
+                if not s:
+                    continue
+                try:
+                    yield json.loads(s)
+                except json.JSONDecodeError as e:
+                    print(f"[WARN] {path}:{ln}: skipping malformed JSONL line: {e}")
+                    skipped += 1
+                    continue
+        # store skipped-line counts
+        self._skipped_counts[os.path.basename(path)] = skipped
